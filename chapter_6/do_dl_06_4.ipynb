{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_34300\\1599870281.py:5: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'd2l_pytorch.d2l' from '..\\\\d2l_pytorch\\\\d2l.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from imp import reload\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import d2l_pytorch.d2l as d2l\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "reload(d2l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics(\"F:\\python_code\\DL\\Datasets\\JayChouLyrics\\jaychou_lyrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "# 从零开始实现\n",
    "\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "print(\"will use\", device)\n",
    "\n",
    "def get_params():\n",
    "  def _one(shape):\n",
    "    ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype = torch.float32)\n",
    "    return torch.nn.Parameter(ts, requires_grad=True)\n",
    "  def _three():\n",
    "    return (_one((num_inputs, num_hiddens)), _one((num_hiddens, num_hiddens)), torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n",
    "  \n",
    "  W_xi, W_hi, b_i = _three()\n",
    "  W_xf, W_hf, b_f = _three()\n",
    "  W_xo, W_ho, b_o = _three()\n",
    "  W_xc, W_hc, b_c = _three()\n",
    "\n",
    "  W_hq = _one((num_hiddens, num_outputs))\n",
    "  b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32),requires_grad=True)\n",
    "  return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "  return (\n",
    "    torch.zeros((batch_size, num_hiddens), device=device),\n",
    "    torch.zeros((batch_size, num_hiddens), device=device),\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据长短记忆的计算表达式定义模型，需要注意的是，只有隐藏状态会传递到输出层，而记忆细胞不参与输出层的计算\n",
    "\n",
    "\n",
    "def lstm(inputs, state, params):\n",
    "  [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params\n",
    "  (H, C) = state\n",
    "  outputs = []\n",
    "  for X in inputs:\n",
    "    I_ = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)\n",
    "    F_ = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)\n",
    "    O_ = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)\n",
    "    C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)\n",
    "    C = F_ * C + I_ * C_tilda\n",
    "    H = O_ * C.tanh()\n",
    "    Y = torch.matmul(H, W_hq) + b_q\n",
    "    outputs.append(Y)\n",
    "\n",
    "  return outputs, (H, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, [\"分开\", \"不分开\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 214.819970, time 1.91 sec\n",
      " - 分开\n",
      "我想你的我的可爱\n",
      "我想想你的我的爱爱\n",
      "我想你的我的可爱\n",
      "我想想你的我的爱爱\n",
      "我想你的我的可爱\n",
      "我想\n",
      " - 不分开\n",
      "我想你的我的可爱\n",
      "我想想你的我的爱爱\n",
      "我想你的我的可爱\n",
      "我想想你的我的爱爱\n",
      "我想你的我的可爱\n",
      "我想\n",
      "epoch 80, perplexity 70.506629, time 1.21 sec\n",
      " - 分开\n",
      "我想你这你 我不要不\n",
      "我不你 你不了 我不不不\n",
      "我说你的你 我不想不你\n",
      "我说你么 我不要多\n",
      "我不你\n",
      " - 不分开\n",
      "我想要这 我不要\n",
      "我不不觉\n",
      "我不要好 我不要\n",
      "不不 我不想\n",
      "我不要这 我不要\n",
      "不不 我不想\n",
      "我不要\n",
      "epoch 120, perplexity 16.585424, time 1.13 sec\n",
      " - 分开\n",
      "我想要这样 我不想\n",
      "不你 \n",
      "快情我想想你\n",
      "你你的让我面著\n",
      "不知不觉\n",
      "我已了这节奏\n",
      "我知好觉\n",
      "我该了\n",
      " - 不分开\n",
      "你知的美 你在红的可爱女人\n",
      "我想你这辈 我不你的可女人\n",
      "你亮 爱怪 在单了停\n",
      "说有你么 我不多难恼\n",
      "epoch 160, perplexity 4.214057, time 1.08 sec\n",
      " - 分开 我想想和汉堡 \n",
      "我想想你的微笑 想想想和和汉堡\n",
      "我想要你的微笑每天都能看到 \n",
      "我知道这里很美但家的\n",
      " - 不分开\n",
      "你知 我想很久久\n",
      "我不能 你想我的难样\n",
      "问通在直不我 抛发到我满的怒火\n",
      "我想揍你已经 却你依依不舍\n"
     ]
    }
   ],
   "source": [
    "d2l.train_and_predict_rnn(\n",
    "  lstm,\n",
    "  get_params,\n",
    "  init_lstm_state,\n",
    "  num_hiddens,\n",
    "  vocab_size,\n",
    "  device,\n",
    "  corpus_indices,\n",
    "  idx_to_char,\n",
    "  char_to_idx,\n",
    "  False,\n",
    "  num_epochs,\n",
    "  num_steps,\n",
    "  lr,\n",
    "  clipping_theta,\n",
    "  batch_size,\n",
    "  pred_period,\n",
    "  pred_len,\n",
    "  prefixes,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

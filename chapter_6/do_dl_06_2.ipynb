{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import d2l_pytorch.d2l as d2l\n",
    "\n",
    "reload(d2l)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics(\n",
    "  \"F:\\python_code\\DL\\Datasets\\JayChouLyrics\\jaychou_lyrics.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, n_class, dtype=torch.float32):\n",
    "  x = x.long()\n",
    "  res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n",
    "  res.scatter_(1, x.view(-1, 1), 1)\n",
    "  return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 2])\n",
    "x.long()\n",
    "# one_hot(x, vocab_size)\n",
    "F.one_hot(x, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1028])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(10).view(2, 5)\n",
    "inputs = d2l.to_onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型参数\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "print(\"will use\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "  def _one(shape):\n",
    "    ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "    return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "  W_xh = _one((num_inputs, num_hiddens))\n",
    "  W_hh = _one((num_hiddens, num_hiddens))\n",
    "  b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "  W_hq = _one((num_hiddens, num_outputs))\n",
    "  b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "  return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "  return (torch.zeros((batch_size, num_hiddens), device=device),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "  W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "  H, = state\n",
    "  outputs = []\n",
    "  for X in inputs:\n",
    "    H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "    Y = torch.matmul(H, W_hq) + b_q\n",
    "    outputs.append(Y)\n",
    "\n",
    "  return outputs, (H,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1028]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "from d2l_pytorch.d2l import to_onehot\n",
    "\n",
    "\n",
    "state = init_rnn_state(X.shape[0], num_hiddens, device)\n",
    "inputs = to_onehot(X.to(device), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预测函数\n",
    "def predict_rnn(\n",
    "  prefix,\n",
    "  num_chars,\n",
    "  rnn,\n",
    "  params,\n",
    "  init_rnn_state,\n",
    "  num_hiddens,\n",
    "  vocab_size,\n",
    "  device,\n",
    "  idx_to_char,\n",
    "  char_to_idx,\n",
    "):\n",
    "  state = init_rnn_state(1, num_hiddens, device)\n",
    "  output = [char_to_idx[prefix[0]]]\n",
    "\n",
    "  for t in range(num_chars + len(prefix) - 1):\n",
    "    X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n",
    "    (Y, state) = rnn(X, state, params)\n",
    "\n",
    "    if t < len(prefix) - 1:\n",
    "      output.append(char_to_idx[prefix[t + 1]])\n",
    "    else:\n",
    "      output.append(int(Y[0].argmax(dim=1).item()))\n",
    "\n",
    "  return \"\".join([idx_to_char[i] for i in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开壶毫刀掩侬晚擅传否甜'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn(\n",
    "  \"分开\", 10, rnn, params, init_rnn_state, num_hiddens, vocab_size, device, idx_to_char, char_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, device):\n",
    "  norm = torch.tensor([0.0], device=device)\n",
    "  for param in params:\n",
    "    norm += (param.grad.data**2).sum()\n",
    "\n",
    "  norm = norm.sqrt().item()\n",
    "  if norm > theta:\n",
    "    for param in params:\n",
    "      param.grad.data *= (theta / norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, [\"分开\",\"不分开\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 77.298071, time 0.46 sec\n",
      " - 分开 我不要再生你的让我疯狂的可爱女人\n",
      "坏坏的让我疯狂的可爱女人\n",
      "坏坏的让我疯狂的可爱女人\n",
      "坏坏的让我疯\n",
      " - 不分开我\n",
      "我有我有女人 一场哈兮 在小村外 在小村外 我有一场 在小村外 我有一场 在小村外 我有一场 在\n",
      "epoch 100, perplexity 9.887331, time 0.39 sec\n",
      " - 分开\n",
      "我已想这样牵着你的手不放开\n",
      "爱可不能够永远单纯没有悲害\n",
      "你 靠着我的肩膀\n",
      "你 想不胸口阳车\n",
      "我说想\n",
      " - 不分开吗\n",
      "我想想你 你着我 别我 我 我不多\n",
      "我给就这样牵\n",
      "后知后觉\n",
      "又已了离 我 已带球\n",
      "我给能这样活\n",
      "\n",
      "epoch 150, perplexity 2.822328, time 0.45 sec\n",
      " - 分开我想妈\n",
      "就小我的让我 一檐是剧\n",
      "我一道的生活 我爱你 你爱我\n",
      "不要了枪\n",
      "喜堡好双截棍 哼哼哈兮\n",
      "快使\n",
      " - 不分开期\n",
      "我想你和 你打我妈\n",
      "这样了我已始\n",
      "没有安我对地的暴\n",
      "别言准备重袭\n",
      "我该念起国小的课桌就\n",
      "用铅一只\n",
      "epoch 200, perplexity 1.566349, time 0.36 sec\n",
      " - 分开的狗萨 问铁变风 全背它纵\n",
      "恨底对中 象一场梦 不敢去碰 你去操\n",
      "连我该轻重 有慢的衷走\n",
      "能失什么映\n",
      " - 不分开扫把的胖女巫 用拉丁文念咒语啦啦呜\n",
      "她养的黑猫笑起来像哭 啦啦啦呜 \n",
      "你的回旧简问 太彻会\n",
      "让我连恨\n",
      "epoch 250, perplexity 1.319662, time 0.50 sec\n",
      " - 分开不 像堡去回忆\n",
      "你在那  你 得下出\n",
      "我手眼的叹息\n",
      "已风安著风\n",
      "一天就到落里来到\n",
      "陷头让危险边缘Ba\n",
      " - 不分开期\n",
      "我叫你爸 你打我妈\n",
      "这样对吗干嘛这样\n",
      "何必让酒牵鼻子走\n",
      "瞎 说了它比谁\n",
      "难领 我爸 我不能再想你\n"
     ]
    }
   ],
   "source": [
    "from d2l_pytorch.d2l import train_and_predict_rnn\n",
    "\n",
    "\n",
    "train_and_predict_rnn(\n",
    "  rnn,\n",
    "  get_params,\n",
    "  init_rnn_state,\n",
    "  num_hiddens,\n",
    "  vocab_size,\n",
    "  device,\n",
    "  corpus_indices,\n",
    "  idx_to_char,\n",
    "  char_to_idx,\n",
    "  True,\n",
    "  num_epochs,\n",
    "  num_steps,\n",
    "  lr,\n",
    "  clipping_theta,\n",
    "  batch_size,\n",
    "  pred_period,\n",
    "  pred_len,\n",
    "  prefixes,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 2, 256]) 1 torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# 简洁实现\n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics(\"F:\\python_code\\DL\\Datasets\\JayChouLyrics\\jaychou_lyrics.txt\")\n",
    "\n",
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(input_size = vocab_size, hidden_size=num_hiddens)\n",
    "num_steps = 35\n",
    "batch_size = 2\n",
    "state = None\n",
    "\n",
    "X = torch.rand(num_steps, batch_size, vocab_size)\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "print(Y.shape, len(state_new), state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "  def __init__(self, rnn_layer, vocab_size):\n",
    "    super(RNNModel, self).__init__()\n",
    "    self.rnn = rnn_layer\n",
    "    self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1)\n",
    "    self.vocab_size = vocab_size\n",
    "    self.dense = nn.Linear(self.hidden_size, vocab_size)\n",
    "    self.state = None\n",
    "\n",
    "  def forward(self, inputs, state):\n",
    "    X = d2l.to_onehot(inputs, self.vocab_size)\n",
    "    Y, self.state = self.rnn(torch.stack(X), state)\n",
    "    output = self.dense(Y.view(-1, Y.shape[-1]))\n",
    "    return output, self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在哀容战战战酿战酿战酿'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(rnn_layer, vocab_size).to(device)\n",
    "d2l.predict_rnn_pytorch(\"在\", 10, model, vocab_size, device, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 50, perplexity 9.016533 , time 0.26 sec \n",
      " - 分开 我不了口想\n",
      "\n",
      "说在那里 哼哼哈兮\n",
      "快使用双截棍 哼哼哈兮\n",
      "快使用双截棍 哼哼哈兮\n",
      "快使用双截棍 哼\n",
      " - 不分开不能再想著你 是我的的爱边河\n",
      "\n",
      "想你的手在着开的可爱\n",
      "人\n",
      "坏坏的让我疯狂的可爱女人\n",
      "坏坏的让我疯狂的\n",
      " epoch 100, perplexity 1.268239 , time 0.10 sec \n",
      " - 分开 我不多不力\n",
      "你说你直在我来不知道你前一切落我不能\n",
      "想通 却又再考倒我\n",
      "说散 你想很久了吧?\n",
      "败给你\n",
      " - 不分开不了就想的你\n",
      "快使用双截棍 哼哼哈兮\n",
      "快使用双截棍 哼哼哈兮\n",
      "如果我有轻功 飞檐走壁\n",
      "为人耿直不屈 \n",
      " epoch 150, perplexity 1.068386 , time 0.11 sec \n",
      " - 分开 我不多不除\n",
      "你爱就直是你\n",
      "想要你 陪我久了太多\n",
      "你说完美就离\n",
      "一直和汉堡 \n",
      "想要你的微笑每天都能看\n",
      " - 不分开不了口不开\n",
      "武它\n",
      "是家庭的没有久\n",
      "我不懂 你你在 别离的太多\n",
      "我想是你是雨是悲剧\n",
      "是说完美演出 一场\n",
      " epoch 200, perplexity 1.034696 , time 0.11 sec \n",
      " - 分开 我不了口让\n",
      "\n",
      "道过我不想我想要你想你\n",
      "一只会说好语举\n",
      "如果真到掉的让我想和你这样\n",
      "快样的回忆对 我\n",
      " - 不分开你了它一口好生\n",
      "快我它的回头 \n",
      "路完美主义 太彻底\n",
      "分手的话像语言暴力\n",
      "我已无能为力再提起 决定中断\n",
      " epoch 250, perplexity 1.023370 , time 0.09 sec \n",
      " - 分开 我不了口让\n",
      "\n",
      "道过我是那我想要看到我\n",
      "\n",
      "想要和你又会听时样\n",
      "是说为没有你在 我不多\n",
      "我天都没有你在\n",
      " - 不分开你听它一开始生活\n",
      "我感的是我场悲剧\n",
      "我可以让生命就这样毫无意义\n",
      "或许在最后能听到你一句\n",
      "轻轻的叹息 \n"
     ]
    }
   ],
   "source": [
    "from d2l_pytorch.d2l import train_and_predict_rnn_pytorch\n",
    "\n",
    "\n",
    "num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 50, [\"分开\", \"不分开\"]\n",
    "train_and_predict_rnn_pytorch(\n",
    "  model,\n",
    "  num_hiddens,\n",
    "  vocab_size,\n",
    "  device,\n",
    "  corpus_indices,\n",
    "  idx_to_char,\n",
    "  char_to_idx,\n",
    "  num_epochs,\n",
    "  num_steps,\n",
    "  lr,\n",
    "  clipping_theta,\n",
    "  batch_size,\n",
    "  pred_period,\n",
    "  pred_len,\n",
    "  prefixes,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

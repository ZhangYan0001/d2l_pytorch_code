{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec 的实现\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import d2l_pytorch.d2l as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 42068\n",
      "# tokens:  24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "# tokens:  15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "# tokens:  11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert \"ptb.train.txt\" in os.listdir(\"../Datasets/PTB\")\n",
    "\n",
    "with open(\"../Datasets/PTB/ptb.train.txt\", \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "  raw_dataset = [st.split() for st in lines]\n",
    "  \n",
    "print(\"# sentences: %d\" % len(raw_dataset))\n",
    "\n",
    "for st in raw_dataset[:3]:\n",
    "  print(\"# tokens: \", len(st), st[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens : 887100 \n"
     ]
    }
   ],
   "source": [
    "# 建立层词语索引\n",
    "# 为了计算简单，只保留在数据集中至少出现5次的词\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))\n",
    "\n",
    "# 然后将词映射到整数索引\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "print(\"# tokens : %d \" % num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens: 375665\n"
     ]
    }
   ],
   "source": [
    "def discard(idx):\n",
    "  return random.uniform(0, 1) < 1 - math.sqrt((1e-4 / counter[idx_to_token[idx]] * num_tokens))\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "\n",
    "print(\"# tokens: %d\" % sum([len(st) for st in subsampled_dataset]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# the: before =50770, after = 2144'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "  return \"# %s: before =%d, after = %d\" % (\n",
    "    token,\n",
    "    sum([st.count(token_to_idx[token]) for st in dataset]),\n",
    "    sum([st.count(token_to_idx[token]) for st in subsampled_dataset]),\n",
    "  )\n",
    "\n",
    "compare_counts(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "  centers, contexts = [], []\n",
    "  for st in dataset:\n",
    "    if len(st) < 2:\n",
    "      continue\n",
    "    centers += st\n",
    "    for center_i in range(len(st)):\n",
    "      window_size = random.randint(1, max_window_size)\n",
    "      indices = list(\n",
    "        range(max(0, center_i - window_size), min(len(st), center_i + 1 + window_size))\n",
    "      )\n",
    "      indices.remove(center_i)\n",
    "      contexts.append([st[idx] for idx in indices])\n",
    "\n",
    "  return centers, contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2, 3]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [3, 4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7,10))]\n",
    "print(\"dataset\", tiny_dataset)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "  print(\"center\", center, \"has contexts\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负采样\n",
    "# 采用负采样来进行近似训练\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "  all_negatives, neg_candidates, i = [], [], 0\n",
    "  population = list(range(len(sampling_weights)))\n",
    "  for contexts in all_contexts:\n",
    "    negatives = []\n",
    "    while len(negatives) < len(contexts) * K:\n",
    "      if i == len(neg_candidates):\n",
    "        i, neg_candidates = 0, random.choices(population, sampling_weights, k=int(1e5))\n",
    "      neg, i = neg_candidates[i], i + 1\n",
    "      if neg not in set(contexts):\n",
    "        negatives.append(neg)\n",
    "    all_negatives.append(negatives)\n",
    "\n",
    "  return all_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_weights = [counter[w] ** 0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, centers, contexts, negatives):\n",
    "    assert len(centers) == len(contexts) == len(negatives)\n",
    "    self.centers = centers\n",
    "    self.contexts = contexts\n",
    "    self.negatives = negatives\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "  \"\"\"\n",
    "  用作DataLoader的参数collate_fn: 输入是个长为batchsize的list,\n",
    "  list中的每个元素都是Dataset类调用__getitem__得到的结果\n",
    "  \"\"\"\n",
    "  max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "  centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "  for center, context, negative in data:\n",
    "    cur_len = len(context) + len(negative)\n",
    "    centers += [center]\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "\n",
    "  return (\n",
    "    torch.tensor(centers).view(-1, 1),\n",
    "    torch.tensor(contexts_negatives),\n",
    "    torch.tensor(masks),\n",
    "    torch.tensor(labels),\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape:  torch.Size([512, 1])\n",
      "contexts_negatives shape:  torch.Size([512, 60])\n",
      "masks shape:  torch.Size([512, 60])\n",
      "labels shape:  torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith(\"win32\") else 4\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=batchify, num_workers=num_workers)\n",
    "\n",
    "for batch in data_iter:\n",
    "  for name, data in zip([\"centers\", \"contexts_negatives\", \"masks\", \"labels\"], batch):\n",
    "    print(name, \"shape: \", data.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.4418,  0.0980, -0.3250, -0.7391],\n",
       "        [-1.7709,  0.9966, -1.5428,  0.1804],\n",
       "        [-0.2097, -0.9861,  1.6934,  0.8230],\n",
       "        [ 0.9468, -0.7926,  0.0564,  1.4849],\n",
       "        [-0.2669, -1.4841, -1.5706,  0.3875],\n",
       "        [ 0.1888,  0.4935,  0.0440,  0.4638],\n",
       "        [ 1.9455, -0.0733, -1.2296,  0.3174],\n",
       "        [-1.4716,  0.9452, -0.4368,  1.0484],\n",
       "        [-0.3472,  0.6737,  1.6569,  0.1197],\n",
       "        [ 1.0856,  0.7494, -0.6048, -0.1343],\n",
       "        [-0.5114, -1.0079, -1.1517,  0.7051],\n",
       "        [ 0.7341, -0.0736,  2.3829,  0.0430],\n",
       "        [ 0.0382,  1.2946, -0.6021,  0.9613],\n",
       "        [ 0.0298,  0.5747,  0.5826, -0.7626],\n",
       "        [-0.3054, -1.0021,  1.3428,  1.1644],\n",
       "        [-1.3099,  0.8449,  1.2868, -0.7026],\n",
       "        [ 0.0469, -1.1069,  0.3855,  0.0371],\n",
       "        [-0.6478, -0.7745, -1.0327, -2.1436],\n",
       "        [-0.0846,  0.9328, -1.8904,  0.1263],\n",
       "        [ 1.3697, -0.8446,  0.2677, -1.5738]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 嵌入层\n",
    "# 获取词嵌入的层称为嵌入层\n",
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "embed.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7709,  0.9966, -1.5428,  0.1804],\n",
       "         [-0.2097, -0.9861,  1.6934,  0.8230],\n",
       "         [ 0.9468, -0.7926,  0.0564,  1.4849]],\n",
       "\n",
       "        [[-0.2669, -1.4841, -1.5706,  0.3875],\n",
       "         [ 0.1888,  0.4935,  0.0440,  0.4638],\n",
       "         [ 1.9455, -0.0733, -1.2296,  0.3174]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2, 1, 4))\n",
    "Y = torch.ones((2, 4, 6))\n",
    "torch.bmm(X, Y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 跳字模型前向计算\n",
    "# 跳字模型的输入包含中心词索引center以及连结的背景词与噪声词索引 contexts_and_negatives\n",
    "# center 变量的性状为(批量大小, 1), 而contexts_and_negatives变量的形状为(批量大小, max_len)\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "  v = embed_v(center)\n",
    "  u = embed_u(contexts_and_negatives)\n",
    "  pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "  return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二元交叉熵损失函数\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "\n",
    "  def forward(self, inputs, targets, mask=None):\n",
    "    \"\"\"\n",
    "    input - Tensor shape: (batch_size, len)\n",
    "    target - Tensor of the same shape as input\n",
    "    \"\"\"\n",
    "    inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "    res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "    return res.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = SigmoidBinaryCrossEntropyLoss()\n",
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "  return -math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "\n",
    "print(\"%.4f\" % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4))\n",
    "print(\"%.4f\" % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "  nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "  nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(net, lr, num_epochs):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(\"train on\", device)\n",
    "  net = net.to(device)\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "  for epoch in range(num_epochs):\n",
    "    start, l_sum, n = time.time(), 0.0, 0\n",
    "    for batch in data_iter:\n",
    "      center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "      pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "\n",
    "      l1 = (\n",
    "        loss(pred.view(label.shape), label, mask) * mask.shape[1] / mask.float().sum(dim=1)\n",
    "      ).mean()\n",
    "      optimizer.zero_grad()\n",
    "      l1.backward()\n",
    "      optimizer.step()\n",
    "      l_sum += l1.cpu().item()\n",
    "      n += 1\n",
    "    print(\"epoch %d, loss %.2f, time %.2fs\" % (epoch + 1, l_sum / n, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 1.97, time 21.07s\n",
      "epoch 2, loss 0.62, time 18.71s\n",
      "epoch 3, loss 0.45, time 18.63s\n",
      "epoch 4, loss 0.40, time 18.79s\n",
      "epoch 5, loss 0.37, time 18.50s\n",
      "epoch 6, loss 0.35, time 18.79s\n",
      "epoch 7, loss 0.34, time 18.78s\n",
      "epoch 8, loss 0.33, time 18.46s\n",
      "epoch 9, loss 0.32, time 18.86s\n",
      "epoch 10, loss 0.32, time 19.16s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.433: doyle\n",
      "cosine sim=0.424: engineering\n",
      "cosine sim=0.419: microprocessor\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "  W = embed.weight.data\n",
    "  x = W[token_to_idx[query_token]]\n",
    "  cos = torch.matmul(W, x) / (torch.sum(W* W, dim=1) * torch.sum(x* x) + 1e-9).sqrt()\n",
    "  _, topk = torch.topk(cos, k=k+1)\n",
    "  topk = topk.cpu().numpy()\n",
    "  for i in topk[1:]:\n",
    "    print(\"cosine sim=%.3f: %s\" % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens(\"chip\", 3, net[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

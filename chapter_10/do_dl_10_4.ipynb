{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_18344\\231235108.py:8: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "# 文本情感分类\n",
    "# 使用卷积神经网络（textCNN）\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "from imp import reload\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import d2l_pytorch.d2l as d2l\n",
    "\n",
    "reload(d2l)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_ROOT = \"..//..//ACLImdb_v1//\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr1d(X, K):\n",
    "  w = K.shape[0]\n",
    "  Y = torch.zeros((X.shape[0] - w +1))\n",
    "  for i in range(Y.shape[0]):\n",
    "    Y[i] = (X[i: i+w] * K).sum()\n",
    "\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  5.,  8., 11., 14., 17.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, K = torch.tensor([0, 1, 2, 3, 4, 5, 6]), torch.tensor([1, 2])\n",
    "corr1d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  8., 14., 20., 26., 32.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr1d_multi_in(X, K):\n",
    "  return torch.stack([corr1d(x, k) for x, k in zip(X, K)]).sum(dim=0)\n",
    "\n",
    "\n",
    "X = torch.tensor(\n",
    "  [\n",
    "    [0, 1, 2, 3, 4, 5, 6],\n",
    "    [1, 2, 3, 4, 5, 6, 7],\n",
    "    [2, 3, 4, 5, 6, 7, 8],\n",
    "  ]\n",
    ")\n",
    "K = torch.tensor([[1, 2], [3, 4], [-1, -3]])\n",
    "corr1d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GlobalMaxPool1d, self).__init__()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # x shape: (batch_size, channel, seq_len)\n",
    "    # return shape: (batch_size, channel, 1)\n",
    "    return F.max_pool1d(x, kernel_size=x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:03<00:00, 4013.38it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 4556.73it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 5362.73it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 5375.49it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_data = d2l.read_imdb(\"train\", data_root=\"..//..//ACLImdb_v1//aclimdb\")\n",
    "test_data = d2l.read_imdb(\"test\", \"..//..//ACLImdb_v1//aclimdb\")\n",
    "\n",
    "vocab = d2l.get_vocab_imdb(train_data)\n",
    "\n",
    "train_set = Data.TensorDataset(*d2l.preprocess_imdb(train_data, vocab))\n",
    "test_set = Data.TensorDataset(*d2l.preprocess_imdb(test_data, vocab))\n",
    "\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建textCNN模型\n",
    "# 模型主要使用了一维卷积层和时序最大池化层\n",
    "# 假设输入的文本序列由n个词组成，每个词用d维的词向量表示。那输入样本的宽为n，高为1，输入通道数为d\n",
    "\n",
    "\"\"\" \n",
    "  1. 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性\n",
    "  2. 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出连结为向量\n",
    "  3. 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "  def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "    super(TextCNN, self).__init__()\n",
    "    self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "\n",
    "    # 不参与训练的嵌入层\n",
    "    self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "\n",
    "    # 时序最大池化层没有权重， 所以可以共用一个实例\n",
    "    self.pool = GlobalMaxPool1d()\n",
    "    self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "\n",
    "    for c, k in zip(num_channels, kernel_sizes):\n",
    "      self.convs.append(nn.Conv1d(in_channels=2 * embed_size, out_channels=c, kernel_size=k))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # 将两个性状是（批量大小，词数，词向量维度）的嵌入层的输出按词向量连结\n",
    "    embeddings = torch.cat((self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "    \n",
    "    # 根据Conv1D要求的输入格式，将词向量维，即一维卷积层的通道维（即词向量那一维），变换到前一维\n",
    "    embeddings = embeddings.permute(0, 2, 1)\n",
    "\n",
    "    #对于每个一维卷积层，在时序最大化池化后会得到一个性状为（批量大小，通道大小，1）的Tensor。使用flatten函数去掉最后一维，然后在通道维上连结\n",
    "    encoding = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "    outputs = self.decoder(self.dropout(encoding))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

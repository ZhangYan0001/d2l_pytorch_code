{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本情感分类\n",
    "# 使用循环神经网络\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "from imp import reload\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import torchtext.vocab as Vocab\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2l_pytorch.d2l as d2l\n",
    "\n",
    "reload(d2l)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_ROOT  = \"..//..//ACLImdb_v1//\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 检验数据集是否存在\n",
    "\n",
    "data_root = DATA_ROOT+\"aclImdb\"\n",
    "print(os.path.exists(data_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_imdb(folder=\"train\", data_root=DATA_ROOT + \"aclimdb\"):\n",
    "  data = []\n",
    "  for label in [\"pos\", \"neg\"]:\n",
    "    folder_name = os.path.join(data_root, folder, label)\n",
    "\n",
    "    for file in tqdm(os.listdir(folder_name)):\n",
    "      with open(os.path.join(folder_name, file), \"rb\") as f:\n",
    "        review = f.read().decode(\"utf-8\").replace(\"\\n\", \"\").lower()\n",
    "        data.append([review, 1 if label == \"pos\" else 0])\n",
    "        \n",
    "  random.shuffle(data)\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:08<00:00, 1501.79it/s]\n",
      "100%|██████████| 12500/12500 [00:08<00:00, 1517.40it/s]\n",
      "100%|██████████| 12500/12500 [00:07<00:00, 1591.05it/s]\n",
      "100%|██████████| 12500/12500 [00:14<00:00, 864.26it/s] \n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = read_imdb(\"train\"),read_imdb(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据\n",
    "\n",
    "# 分词\n",
    "def get_tokenized_imdb(data):\n",
    "  \"\"\"\n",
    "  data: list of [string, label]\n",
    "  \"\"\"\n",
    "  def tokenizer(text):\n",
    "    return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "  return [tokenizer(review) for review, _ in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 46152\n"
     ]
    }
   ],
   "source": [
    "# 创建字典\n",
    "def get_vocab_imdb(data):\n",
    "  tokenized_data = get_tokenized_imdb(data)\n",
    "  counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "  return Vocab.Vocab(counter, min_freq=5)\n",
    "\n",
    "vocab = get_vocab_imdb(train_data)\n",
    "print(\"# words in vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每条评论长度不一致所以不能直接合成小批量\n",
    "# 需要对每条评论进行分词，并通过字典转换成词索引，通过截断或补0来将每条评论长度固定到500\n",
    "\n",
    "\n",
    "def preprocess_imdb(data, vocab):\n",
    "  max_l = 500\n",
    "\n",
    "  def pad(x):\n",
    "    return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "\n",
    "  tokenized_data = get_tokenized_imdb(data)\n",
    "  features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "  labels = torch.tensor([score for _, score in data])\n",
    "  return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n",
      "# batches:  391\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_set = Data.TensorDataset(*preprocess_imdb(train_data, vocab))\n",
    "\n",
    "test_set = Data.TensorDataset(*preprocess_imdb(test_data,vocab))\n",
    "\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_set, batch_size)\n",
    "\n",
    "for X, y in train_iter:\n",
    "  print(\"X\", X.shape, \"y\", y.shape)\n",
    "  break\n",
    "\n",
    "print(\"# batches: \", len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建RNN循环神经网络\n",
    "\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "  def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "    super(BiRNN, self).__init__()\n",
    "    self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "    self.encoder = nn.LSTM(\n",
    "      input_size=embed_size, hidden_size=num_hiddens, num_layers=num_layers, bidirectional=True\n",
    "    )\n",
    "    self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    # inputs的形状是(批量大小，词数), 因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后再提取词特征\n",
    "    # 输出性状为(词数，批量大小，词向量维度)\n",
    "    embeddings = self.embedding(inputs.permute(1, 0))\n",
    "    # rnn.LSTM只传入输入embeddings, 因此只返回最后一层的隐藏层在各时间步的隐藏状态\n",
    "    # outputs形状是（词数，批量大小，2*隐藏单元个数）\n",
    "    outputs, _ = self.encoder(embeddings)\n",
    "    # output , (h, c)\n",
    "    # 连接初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为 （批量大小，4*隐藏单元个数）\n",
    "    encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "    outs = self.decoder(encoding)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:31<00:00, 12800.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# 情感分类的训练数据集不是很大，为了应对过拟合，直接使用更大规模的语料预训练的词向量作为每个词的特征向量\n",
    "glove_vocab = Vocab.GloVe(name=\"6B\", dim=100, cache=os.path.join(\"../Datasets/Glove\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21202 oov words.\n"
     ]
    }
   ],
   "source": [
    "from d2l_pytorch.d2l import load_pretrained_embedding\n",
    "\n",
    "\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.5667, train acc 0.703, test acc 0.775 , time 49.1 sec\n",
      "epoch 2, loss 0.2198, train acc 0.798, test acc 0.807 , time 45.5 sec\n",
      "epoch 3, loss 0.1239, train acc 0.839, test acc 0.773 , time 45.8 sec\n",
      "epoch 4, loss 0.0843, train acc 0.858, test acc 0.849 , time 45.9 sec\n",
      "epoch 5, loss 0.0578, train acc 0.879, test acc 0.854 , time 46.2 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2l.predict_sentiment(net, vocab, [\"this\", \"movie\", \"is\",\"so\",\"great\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2l.predict_sentiment(net, vocab, [\"this\", \"movie\", \"is\", \"so\", \"bad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

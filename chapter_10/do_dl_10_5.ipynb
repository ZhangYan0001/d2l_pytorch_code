{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器-解码器（seq2seq)\n",
    "# 在自然语言处理中输入和输出都可以是不定长序列\n",
    "# 当输入和输出都是不定长序列时，我们可以使用编码器-解码器(encoder-decoder)和seq2seq模型，这两个模型本质上都用到了两个循环神经网络，分别叫做编码器和解码器\n",
    "\n",
    "# 在训练数据集中每个句子后附上特殊符号<eos>(end of sequence),以表示序列的终止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 机器翻译\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import d2l_pytorch.d2l as d2l\n",
    "\n",
    "PAD, BOS, EOS = \"<pad>\", \"<bos>\", \"<eos>\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义两个辅助函数对后面读取的数据进行预处理\n",
    "\n",
    "# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "# 长度变为max_seq_len,然后将序列保存在all_seqs中\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "  all_tokens.extend(seq_tokens)\n",
    "  seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "  all_seqs.append(seq_tokens)\n",
    "\n",
    "\n",
    "# 使用所有的词来构建词典，并将所有序列中的词变换为词索引后构造Tensor\n",
    "def build_data(all_tokens, all_seqs):\n",
    "  vocab = Vocab.Vocab(collections.Counter(all_tokens), specials=[PAD, BOS, EOS])\n",
    "  indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs]\n",
    "  return vocab, torch.tensor(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一个很小的法语——英语数据集\n",
    "# '\\t'隔开， 在句末附上“<EOS>”，添加“<PAD>”\n",
    "def read_data(max_seq_len):\n",
    "  in_tokens, out_tokens, in_seqs, out_seqs = [], [], [], []\n",
    "\n",
    "  with open(\"../Datasets/fr-en-small.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  for line in lines:\n",
    "    in_seq, out_seq = line.rstrip().split(\"\\t\")\n",
    "    in_seq_tokens, out_seq_tokens = in_seq.split(\" \"), out_seq.split(\" \")\n",
    "\n",
    "    if max(len(in_seq_tokens), len(out_seq_tokens)) > max_seq_len - 1:\n",
    "      continue\n",
    "\n",
    "    process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)\n",
    "    process_one_seq(out_seq_tokens, out_tokens, out_seqs, max_seq_len)\n",
    "\n",
    "  in_vocab, in_data = build_data(in_tokens, in_seqs)\n",
    "  out_vocab, out_data = build_data(out_tokens, out_seqs)\n",
    "  return in_vocab, out_vocab, Data.TensorDataset(in_data, out_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5,  4, 45,  3,  2,  0,  0]), tensor([ 8,  4, 27,  3,  2,  0,  0]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 7\n",
    "in_vocab, out_vocab, dataset = read_data(max_seq_len)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 含注意力机制的编码器-解码器\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  \"\"\"\n",
    "  在编码器中，将输入语言的词索引通过词嵌入层得到词的表征，然后输入到一个多层门控循环单元中\n",
    "\n",
    "  pytorch中的nn.GRU实例在前向计算后也会分别返回输出和最终时间步的多层隐藏状态。\n",
    "  其中的输出指的是最后一层的隐藏层在各个时间步的隐藏状态，并不涉及输出层计算\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, drop_prob=0, **kwargs):\n",
    "    super(Encoder, self).__init__(**kwargs)\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=drop_prob)\n",
    "\n",
    "  def forward(self, inputs, state):\n",
    "    # 输入性状是（批量大小，时间步数）。将输出互换样本维和时间步维\n",
    "    embedding = self.embedding(inputs.long()).permute(1, 0, 2)  # (seq_len, batch, input_size)\n",
    "    return self.rnn(embedding, state)\n",
    "\n",
    "  def begin_state(self):\n",
    "    return None  # 隐藏态初始化为None时PyTorch会自动初始化为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoder = Encoder(vocab_size = 10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "output, state = encoder(torch.zeros((4, 7)), encoder.begin_state())\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力机制\n",
    "def attention_model(input_size, attention_size):\n",
    "  model = nn.Sequential(\n",
    "    nn.Linear(input_size, attention_size, bias=False),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(attention_size, 1, bias=False),\n",
    "  )\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_forward(model, enc_states, dec_state):\n",
    "  \"\"\"\n",
    "  enc_states: (时间步数，批量大小，隐藏单元个数)\n",
    "  dec_state: (批量大小，隐藏单元个数)\n",
    "  \"\"\"\n",
    "  # 将解码器隐藏状态广播到和编码器隐藏状态性状相同后进行连结\n",
    "  dec_states = dec_state.unsqueeze(dim=0).expand_as(enc_states)\n",
    "  enc_and_dec_states = torch.cat((enc_states, dec_states), dim=2)\n",
    "  e = model(enc_and_dec_states) # 性状为（时间步数，批量大小，1）\n",
    "  alpha = F.softmax(e, dim=0) # 在时间步维度做softmax运算\n",
    "  return (alpha * enc_states).sum(dim=0) # 返回背景变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len, batch_size, num_hiddens = 10, 4, 8\n",
    "model = attention_model(2*num_hiddens, 10)\n",
    "enc_states = torch.zeros((seq_len, batch_size, num_hiddens))\n",
    "dec_state = torch.zeros((batch_size, num_hiddens))\n",
    "attention_forward(model, enc_states, dec_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(\n",
    "    self, vocab_size, embed_size, num_hiddens, num_layers, attention_size, drop_prob=0\n",
    "  ) -> None:\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.attention = attention_model(2 * num_hiddens, attention_size)\n",
    "    self.rnn = nn.GRU(num_hiddens + embed_size, num_hiddens, num_layers, dropout=drop_prob)\n",
    "    self.out = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "  def forward(self, cur_input, state, enc_states):\n",
    "    \"\"\"\n",
    "    cur_input shape:(batch, )\n",
    "    state shape: (num_layers, batch, num_hiddens)\n",
    "    \"\"\"\n",
    "    c = attention_forward(self.attention, enc_states, state[-1])\n",
    "    input_and_c = torch.cat((self.embedding(cur_input), c), dim=1)\n",
    "    output, state = self.rnn(input_and_c.unsqueeze(0),state)\n",
    "    output = self.out(output).squeeze(dim=0)\n",
    "    return output, state\n",
    "  \n",
    "  def begin_state(self, enc_state):\n",
    "    return enc_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(encoder, decoder, X, Y, loss):\n",
    "  batch_size = X.shape[0]\n",
    "  enc_state = encoder.begin_state()\n",
    "  enc_outputs, enc_state = encoder(X, enc_state)\n",
    "  dec_state = decoder.begin_state(enc_state)\n",
    "  dec_input = torch.tensor([out_vocab.stoi[BOS]] * batch_size)\n",
    "\n",
    "  mask, num_not_pad_tokens = (\n",
    "    torch.ones(\n",
    "      batch_size,\n",
    "    ),\n",
    "    0,\n",
    "  )\n",
    "\n",
    "  l = torch.tensor([0.0])\n",
    "  for y in Y.permute(1, 0):\n",
    "    dec_output, dec_state = decoder(dec_input, dec_state, enc_outputs)\n",
    "    l = l + (mask * loss(dec_output, y)).sum()\n",
    "    dec_input = y\n",
    "    num_not_pad_tokens += mask.sum().item()\n",
    "    mask = mask * (y != out_vocab.stoi[EOS]).float()\n",
    "\n",
    "  return l / num_not_pad_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, lr, batch_size, num_epochs):\n",
    "  enc_optimizer = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
    "  dec_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "  loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "  data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "  for epoch in range(num_epochs):\n",
    "    l_sum = 0.0\n",
    "    for X, Y in data_iter:\n",
    "      enc_optimizer.zero_grad()\n",
    "      dec_optimizer.zero_grad()\n",
    "      l = batch_loss(encoder, decoder, X, Y, loss)\n",
    "      l.backward()\n",
    "      enc_optimizer.step()\n",
    "      dec_optimizer.step()\n",
    "      l_sum += l.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "      print(\"epoch %d, loss %.3f\" % (epoch + 1, l_sum / len(data_iter)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 0.438\n",
      "epoch 20, loss 0.213\n",
      "epoch 30, loss 0.111\n",
      "epoch 40, loss 0.067\n",
      "epoch 50, loss 0.034\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers = 64, 64, 2\n",
    "attention_size, drop_prob, lr, batch_size, num_epochs = 10, 0.5, 0.01, 2, 50\n",
    "encoder = Encoder(len(in_vocab), embed_size, num_hiddens, num_layers, drop_prob)\n",
    "decoder = Decoder(len(out_vocab), embed_size, num_hiddens, num_layers, attention_size, drop_prob)\n",
    "\n",
    "train(encoder, decoder, dataset, lr, batch_size, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(encoder:Encoder, edcoder:Decoder, input_seq, max_seq_len):\n",
    "  in_tokens = input_seq.split(\" \")\n",
    "  in_tokens += [EOS] + [PAD] * (max_seq_len - len(in_tokens) -1)\n",
    "  enc_input = torch.tensor([[in_vocab.stoi[tk] for tk in in_tokens]])\n",
    "  enc_state = encoder.begin_state()\n",
    "  enc_output, enc_state = encoder(enc_input, enc_state)\n",
    "  dec_input = torch.tensor([out_vocab.stoi[BOS]])\n",
    "  dec_state = decoder.begin_state(enc_state)\n",
    "  output_tokens = []\n",
    "\n",
    "  for _ in range(max_seq_len):\n",
    "    dec_output, dec_state = decoder(dec_input, dec_state, enc_output)\n",
    "    pred = dec_output.argmax(dim=1)\n",
    "    pred_token = out_vocab.itos[int(pred.item())]\n",
    "\n",
    "    if pred_token == EOS:\n",
    "      break\n",
    "    else:\n",
    "      output_tokens.append(pred_token)\n",
    "      dec_input = pred\n",
    "      \n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = \"elle est vieille .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she', 'is', 'old', '.']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(encoder, decoder, input_seq, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_tokens, label_tokens, k):\n",
    "  len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "  score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "  for n in range(1, k + 1):\n",
    "    num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "    for i in range(len_label - n + 1):\n",
    "      label_subs[\"\".join(label_tokens[i : i + n])] += 1\n",
    "    for i in range(len_pred - n + 1):\n",
    "      if label_subs[\"\".join(pred_tokens[i : i + n])] > 0:\n",
    "        num_matches += 1\n",
    "        label_subs[\"\".join(pred_tokens[i : i + n])] -= 1\n",
    "    score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "  return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(input_seq, label_seq, k):\n",
    "  pred_tokens = translate(encoder, decoder, input_seq, max_seq_len)\n",
    "  label_tokens = label_seq.split(\" \")\n",
    "  print(\"bleu %.3f, predict: %s\" % (bleu(pred_tokens, label_tokens, k), \" \".join(pred_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu 1.000, predict: they are watching .\n"
     ]
    }
   ],
   "source": [
    "score(\"ils regardent .\", \"they are watching .\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu 0.658, predict: they are russian .\n"
     ]
    }
   ],
   "source": [
    "score(\"ils sont canadienne .\", \"they are canadian .\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
